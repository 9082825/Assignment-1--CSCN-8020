{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1f7c9b2",
   "metadata": {},
   "source": [
    "# CSCN8020 - Assignment 1:\n",
    "\n",
    "\n",
    "# Problem 1\n",
    "## Pick-and-Place Robot MDP Design\n",
    "\n",
    "### Problem Statement\n",
    "Design a reinforcement learning problem as an MDP for controlling a robot arm in a repetitive pick-and-place task. The goal is to learn movements that are fast and smooth through direct motor control.\n",
    "\n",
    "---\n",
    "\n",
    "### MDP Components\n",
    "\n",
    "#### States (S)\n",
    "The state space captures all information necessary for effective control decisions:\n",
    "\n",
    "- **Joint angles**: θ = (θ₁, θ₂, ..., θₙ) for each joint in the arm\n",
    "- **Joint angular velocities**: θ̇ = (θ̇₁, θ̇₂, ..., θ̇ₙ) for each joint\n",
    "- **End-effector position**: (x, y, z) in 3D space\n",
    "- **Gripper state**: (open/closed)\n",
    "- **Object status**: (held/not held)\n",
    "- **Target position**: Current pick or place location\n",
    "\n",
    "**Reasoning**: Positions tell us where the arm is located, while velocities are essential for achieving smooth motion and avoiding jerky movements. Task status information (gripper and object states) helps the agent know whether to pick or place. Since the objective emphasizes smooth movements, velocity information is crucial for the learning process.\n",
    "\n",
    "---\n",
    "\n",
    "#### Actions (A)\n",
    "The action space consists of continuous motor commands:\n",
    "\n",
    "- **Motor torques**: τ = (τ₁, τ₂, ..., τₙ) applied to each joint\n",
    "- **Gripper command**: (open/close/hold)\n",
    "\n",
    "**Reasoning**: Since we're controlling motors directly, actions should be torque commands or voltage signals to each motor. Continuous actions allow for smooth, precise control, which directly aligns with the smoothness objective. This low-level control gives the agent the flexibility to learn optimal trajectories.\n",
    "\n",
    "---\n",
    "\n",
    "#### Rewards (R)\n",
    "A composite reward function that balances speed, smoothness, and task completion:\n",
    "\n",
    "**R(s, a, s') = R_task + R_speed + R_smoothness + R_energy**\n",
    "\n",
    "**Components:**\n",
    "\n",
    "1. **Task Rewards (R_task)**:\n",
    "   - +100 for successful object pick\n",
    "   - +100 for successful object placement\n",
    "   - -1 per timestep (encourages faster task completion)\n",
    "\n",
    "2. **Speed Rewards (R_speed)**:\n",
    "   - -0.1 × time_elapsed (encourages faster movements)\n",
    "\n",
    "3. **Smoothness Rewards (R_smoothness)**:\n",
    "   - -α × ||a - a_prev||² (penalizes sudden changes in motor commands)\n",
    "\n",
    "4. **Energy Rewards (R_energy)**:\n",
    "   - -β × ||a||² (penalizes excessive motor torque, encourages efficiency)\n",
    "\n",
    "**Additional Penalties:**\n",
    "- -50 for dropping the object\n",
    "- -10 for collisions with workspace boundaries\n",
    "- -5 for excessive joint velocities\n",
    "\n",
    "**Reasoning**: The reward structure directly addresses the learning objectives. Task completion rewards ensure the robot fulfills its function. Time penalties encourage speed. Smoothness penalties (penalizing large changes between consecutive actions) promote smooth trajectories by discouraging abrupt motor command changes. Energy penalties prevent wasteful movements and protect the hardware. This multi-objective reward naturally guides the agent toward desired behaviors.\n",
    "\n",
    "---\n",
    "\n",
    "#### Transition Dynamics (P)\n",
    "The transition function P(s'|s, a) is determined by the physics of the robot arm:\n",
    "\n",
    "- **Forward dynamics equations**: How applied torques affect joint accelerations\n",
    "- **Kinematic constraints**: Joint limits and workspace boundaries\n",
    "- **Contact dynamics**: Gripper-object interaction physics\n",
    "- **Environmental factors**: Gravity, friction, and inertia\n",
    "\n",
    "**Reasoning**: In model-free RL, we may not explicitly model these dynamics, but the environment simulator must encode the physical laws governing robot motion. The deterministic or stochastic nature depends on factors like sensor noise and environmental uncertainty.\n",
    "\n",
    "---\n",
    "\n",
    "### Additional Design Considerations\n",
    "\n",
    "1. **Discount Factor (γ)**: Use γ ≈ 0.95-0.99 to balance immediate rewards (speed) with long-term trajectory smoothness\n",
    "\n",
    "2. **Episode Structure**: Each episode starts with the object at the pick location and ends when successfully placed at the target or after a maximum timeout\n",
    "\n",
    "3. **State Representation**: For real implementation, continuous states would likely use function approximation (neural networks) rather than discretization\n",
    "\n",
    "4. **Action Space**: Continuous control is preferred, potentially using algorithms like DDPG, TD3, or SAC\n",
    "\n",
    "---\n",
    "\n",
    "### Talking Points:\n",
    "\n",
    "For this pick-and-place robot task, I've designed an MDP that enables learning of fast and smooth movements through direct motor control.\n",
    "\n",
    "The **state space** includes joint angles, joint velocities, end-effector position, gripper state, and object status. Including velocities is critical because smooth motion depends on understanding movement dynamics, not just positions.\n",
    "\n",
    "**Actions** are continuous motor torques applied to each joint, plus gripper commands. This low-level control allows the agent to learn precise, smooth trajectories.\n",
    "\n",
    "The **reward function** balances multiple objectives: task completion bonuses (+100 for successful pick/place), time penalties to encourage speed (-1 per timestep), smoothness penalties that discourage abrupt motor command changes (-α × ||a - a_prev||²), and energy penalties to prevent excessive torques. This structure directly incentivizes the desired behavior.\n",
    "\n",
    "**Transition dynamics** follow the physical laws governing the robot arm—how torques affect joint motion and object interactions.\n",
    "\n",
    "This formulation provides sufficient information for learning while the reward design naturally guides the agent toward efficient, smooth pick-and-place behaviors.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf737b02",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "## Value Iteration on 2×2 Gridworld (Manual Calculation)\n",
    "\n",
    "### Problem Statement\n",
    "Perform two iterations of Value Iteration for a 2×2 gridworld environment. Show the step-by-step process (without code) including policy evaluation and policy improvement.\n",
    "\n",
    "---\n",
    "\n",
    "### Environment Setup\n",
    "\n",
    "#### Grid Layout\n",
    "```\n",
    "[s1: R=5]   [s2: R=10]\n",
    "[s3: R=1]   [s4: R=2]\n",
    "```\n",
    "\n",
    "#### Environment Characteristics\n",
    "\n",
    "**State Space (S)**: {s1, s2, s3, s4}\n",
    "\n",
    "**Action Space (A)**: {up, down, left, right}\n",
    "\n",
    "**Initial Policy (π)**: For all states, π(up|s) = 1\n",
    "\n",
    "**Transition Probabilities P(s'|s, a)**:\n",
    "- If the action is valid (does not run into a wall), the transition is deterministic\n",
    "- Otherwise, s' = s (agent stays in current state)\n",
    "\n",
    "**Rewards R(s)**:\n",
    "- R(s1) = 5 for all actions a\n",
    "- R(s2) = 10 for all actions a\n",
    "- R(s3) = 1 for all actions a\n",
    "- R(s4) = 2 for all actions a\n",
    "\n",
    "**Discount Factor**: γ = 0.9 (assumed)\n",
    "\n",
    "---\n",
    "\n",
    "### Value Iteration Algorithm\n",
    "\n",
    "The Bellman optimality equation used for value iteration:\n",
    "\n",
    "\\[ V(s) = \\max_a \\sum_{s'} P(s'|s,a) [R(s) + \\gamma V(s')] \\]\n",
    "\n",
    "For deterministic transitions, this simplifies to:\n",
    "\n",
    "\\[ V(s) = \\max_a [R(s) + \\gamma V(s')] \\]\n",
    "\n",
    "---\n",
    "\n",
    "## Iteration 0: Initial Value Function\n",
    "\n",
    "All states start with zero value:\n",
    "\n",
    "| State | V₀(s) |\n",
    "|-------|-------|\n",
    "| s1    | 0.0   |\n",
    "| s2    | 0.0   |\n",
    "| s3    | 0.0   |\n",
    "| s4    | 0.0   |\n",
    "\n",
    "**Grid Visualization:**\n",
    "```\n",
    "[0.0]  [0.0]\n",
    "[0.0]  [0.0]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Iteration 1: First Value Update\n",
    "\n",
    "### State s1 (Top-Left, R=5)\n",
    "\n",
    "Calculate value for each action:\n",
    "\n",
    "- **up**: Hits wall → stays at s1\n",
    "  - V = R(s1) + γ × V₀(s1) = 5 + 0.9(0) = **5.0**\n",
    "\n",
    "- **down**: Moves to s3\n",
    "  - V = R(s1) + γ × V₀(s3) = 5 + 0.9(0) = **5.0**\n",
    "\n",
    "- **left**: Hits wall → stays at s1\n",
    "  - V = R(s1) + γ × V₀(s1) = 5 + 0.9(0) = **5.0**\n",
    "\n",
    "- **right**: Moves to s2\n",
    "  - V = R(s1) + γ × V₀(s2) = 5 + 0.9(0) = **5.0**\n",
    "\n",
    "**V₁(s1) = max(5.0, 5.0, 5.0, 5.0) = 5.0**\n",
    "\n",
    "---\n",
    "\n",
    "### State s2 (Top-Right, R=10)\n",
    "\n",
    "Calculate value for each action:\n",
    "\n",
    "- **up**: Hits wall → stays at s2\n",
    "  - V = R(s2) + γ × V₀(s2) = 10 + 0.9(0) = **10.0**\n",
    "\n",
    "- **down**: Moves to s4\n",
    "  - V = R(s2) + γ × V₀(s4) = 10 + 0.9(0) = **10.0**\n",
    "\n",
    "- **left**: Moves to s1\n",
    "  - V = R(s2) + γ × V₀(s1) = 10 + 0.9(0) = **10.0**\n",
    "\n",
    "- **right**: Hits wall → stays at s2\n",
    "  - V = R(s2) + γ × V₀(s2) = 10 + 0.9(0) = **10.0**\n",
    "\n",
    "**V₁(s2) = max(10.0, 10.0, 10.0, 10.0) = 10.0**\n",
    "\n",
    "---\n",
    "\n",
    "### State s3 (Bottom-Left, R=1)\n",
    "\n",
    "Calculate value for each action:\n",
    "\n",
    "- **up**: Moves to s1\n",
    "  - V = R(s3) + γ × V₀(s1) = 1 + 0.9(0) = **1.0**\n",
    "\n",
    "- **down**: Hits wall → stays at s3\n",
    "  - V = R(s3) + γ × V₀(s3) = 1 + 0.9(0) = **1.0**\n",
    "\n",
    "- **left**: Hits wall → stays at s3\n",
    "  - V = R(s3) + γ × V₀(s3) = 1 + 0.9(0) = **1.0**\n",
    "\n",
    "- **right**: Moves to s4\n",
    "  - V = R(s3) + γ × V₀(s4) = 1 + 0.9(0) = **1.0**\n",
    "\n",
    "**V₁(s3) = max(1.0, 1.0, 1.0, 1.0) = 1.0**\n",
    "\n",
    "---\n",
    "\n",
    "### State s4 (Bottom-Right, R=2)\n",
    "\n",
    "Calculate value for each action:\n",
    "\n",
    "- **up**: Moves to s2\n",
    "  - V = R(s4) + γ × V₀(s2) = 2 + 0.9(0) = **2.0**\n",
    "\n",
    "- **down**: Hits wall → stays at s4\n",
    "  - V = R(s4) + γ × V₀(s4) = 2 + 0.9(0) = **2.0**\n",
    "\n",
    "- **left**: Moves to s3\n",
    "  - V = R(s4) + γ × V₀(s3) = 2 + 0.9(0) = **2.0**\n",
    "\n",
    "- **right**: Hits wall → stays at s4\n",
    "  - V = R(s4) + γ × V₀(s4) = 2 + 0.9(0) = **2.0**\n",
    "\n",
    "**V₁(s4) = max(2.0, 2.0, 2.0, 2.0) = 2.0**\n",
    "\n",
    "---\n",
    "\n",
    "### Updated Value Function After Iteration 1\n",
    "\n",
    "| State | V₁(s) |\n",
    "|-------|-------|\n",
    "| s1    | 5.0   |\n",
    "| s2    | 10.0  |\n",
    "| s3    | 1.0   |\n",
    "| s4    | 2.0   |\n",
    "\n",
    "**Grid Visualization:**\n",
    "```\n",
    "[5.0]   [10.0]\n",
    "[1.0]   [2.0]\n",
    "```\n",
    "\n",
    "**Observation**: After the first iteration, the values equal the immediate rewards since all successor state values were zero.\n",
    "\n",
    "---\n",
    "\n",
    "## Iteration 2: Second Value Update\n",
    "\n",
    "Now we use V₁ values from Iteration 1 to update each state.\n",
    "\n",
    "### State s1 (Top-Left, R=5)\n",
    "\n",
    "Calculate value for each action:\n",
    "\n",
    "- **up**: Hits wall → stays at s1\n",
    "  - V = R(s1) + γ × V₁(s1) = 5 + 0.9(5.0) = 5 + 4.5 = **9.5**\n",
    "\n",
    "- **down**: Moves to s3\n",
    "  - V = R(s1) + γ × V₁(s3) = 5 + 0.9(1.0) = 5 + 0.9 = **5.9**\n",
    "\n",
    "- **left**: Hits wall → stays at s1\n",
    "  - V = R(s1) + γ × V₁(s1) = 5 + 0.9(5.0) = 5 + 4.5 = **9.5**\n",
    "\n",
    "- **right**: Moves to s2\n",
    "  - V = R(s1) + γ × V₁(s2) = 5 + 0.9(10.0) = 5 + 9.0 = **14.0** ✓\n",
    "\n",
    "**V₂(s1) = max(9.5, 5.9, 9.5, 14.0) = 14.0**\n",
    "\n",
    "**Best Action**: right (move to s2)\n",
    "\n",
    "---\n",
    "\n",
    "### State s2 (Top-Right, R=10)\n",
    "\n",
    "Calculate value for each action:\n",
    "\n",
    "- **up**: Hits wall → stays at s2\n",
    "  - V = R(s2) + γ × V₁(s2) = 10 + 0.9(10.0) = 10 + 9.0 = **19.0** ✓\n",
    "\n",
    "- **down**: Moves to s4\n",
    "  - V = R(s2) + γ × V₁(s4) = 10 + 0.9(2.0) = 10 + 1.8 = **11.8**\n",
    "\n",
    "- **left**: Moves to s1\n",
    "  - V = R(s2) + γ × V₁(s1) = 10 + 0.9(5.0) = 10 + 4.5 = **14.5**\n",
    "\n",
    "- **right**: Hits wall → stays at s2\n",
    "  - V = R(s2) + γ × V₁(s2) = 10 + 0.9(10.0) = 10 + 9.0 = **19.0** ✓\n",
    "\n",
    "**V₂(s2) = max(19.0, 11.8, 14.5, 19.0) = 19.0**\n",
    "\n",
    "**Best Actions**: up or right (both keep agent in s2 or hit wall)\n",
    "\n",
    "---\n",
    "\n",
    "### State s3 (Bottom-Left, R=1)\n",
    "\n",
    "Calculate value for each action:\n",
    "\n",
    "- **up**: Moves to s1\n",
    "  - V = R(s3) + γ × V₁(s1) = 1 + 0.9(5.0) = 1 + 4.5 = **5.5** ✓\n",
    "\n",
    "- **down**: Hits wall → stays at s3\n",
    "  - V = R(s3) + γ × V₁(s3) = 1 + 0.9(1.0) = 1 + 0.9 = **1.9**\n",
    "\n",
    "- **left**: Hits wall → stays at s3\n",
    "  - V = R(s3) + γ × V₁(s3) = 1 + 0.9(1.0) = 1 + 0.9 = **1.9**\n",
    "\n",
    "- **right**: Moves to s4\n",
    "  - V = R(s3) + γ × V₁(s4) = 1 + 0.9(2.0) = 1 + 1.8 = **2.8**\n",
    "\n",
    "**V₂(s3) = max(5.5, 1.9, 1.9, 2.8) = 5.5**\n",
    "\n",
    "**Best Action**: up (move to s1)\n",
    "\n",
    "---\n",
    "\n",
    "### State s4 (Bottom-Right, R=2)\n",
    "\n",
    "Calculate value for each action:\n",
    "\n",
    "- **up**: Moves to s2\n",
    "  - V = R(s4) + γ × V₁(s2) = 2 + 0.9(10.0) = 2 + 9.0 = **11.0** ✓\n",
    "\n",
    "- **down**: Hits wall → stays at s4\n",
    "  - V = R(s4) + γ × V₁(s4) = 2 + 0.9(2.0) = 2 + 1.8 = **3.8**\n",
    "\n",
    "- **left**: Moves to s3\n",
    "  - V = R(s4) + γ × V₁(s3) = 2 + 0.9(1.0) = 2 + 0.9 = **2.9**\n",
    "\n",
    "- **right**: Hits wall → stays at s4\n",
    "  - V = R(s4) + γ × V₁(s4) = 2 + 0.9(2.0) = 2 + 1.8 = **3.8**\n",
    "\n",
    "**V₂(s4) = max(11.0, 3.8, 2.9, 3.8) = 11.0**\n",
    "\n",
    "**Best Action**: up (move to s2)\n",
    "\n",
    "---\n",
    "\n",
    "### Updated Value Function After Iteration 2\n",
    "\n",
    "| State | V₂(s) | Best Action |\n",
    "|-------|-------|-------------|\n",
    "| s1    | 14.0  | right       |\n",
    "| s2    | 19.0  | up/right    |\n",
    "| s3    | 5.5   | up          |\n",
    "| s4    | 11.0  | up          |\n",
    "\n",
    "**Grid Visualization:**\n",
    "```\n",
    "[14.0]  [19.0]\n",
    "[5.5]   [11.0]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| State | V₀ | V₁  | V₂   | Optimal Action |\n",
    "|-------|-----|-----|------|----------------|\n",
    "| s1    | 0.0 | 5.0 | 14.0 | right          |\n",
    "| s2    | 0.0 | 10.0| 19.0 | up/right       |\n",
    "| s3    | 0.0 | 1.0 | 5.5  | up             |\n",
    "| s4    | 0.0 | 2.0 | 11.0 | up             |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Observations\n",
    "\n",
    "1. **Value Propagation**: Values increase with each iteration as the agent learns to account for future rewards through the discount factor.\n",
    "\n",
    "2. **Optimal Behavior Emerging**: After two iterations, the policy is gravitating toward state s2 (highest immediate reward of 10), with:\n",
    "   - s1 → right (to s2)\n",
    "   - s3 → up (to s1, then eventually to s2)\n",
    "   - s4 → up (directly to s2)\n",
    "   - s2 → stay (up or right keeps agent in high-reward state)\n",
    "\n",
    "3. **Convergence**: The values are still changing, indicating that more iterations would be needed to reach the optimal value function V*.\n",
    "\n",
    "4. **Policy Improvement**: The initial policy (all states go up) was suboptimal. After two iterations of value iteration, we've identified better actions that maximize expected cumulative reward.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Process\n",
    "\n",
    "The value iteration process follows this pattern:\n",
    "\n",
    "**Iteration 1**: Values reflect only immediate rewards\n",
    "\\[ V₁(s) = R(s) + \\gamma × 0 = R(s) \\]\n",
    "\n",
    "**Iteration 2**: Values incorporate one-step lookahead\n",
    "\\[ V₂(s) = \\max_a [R(s) + \\gamma × V₁(s')] \\]\n",
    "\n",
    "**Subsequent Iterations**: Continue until convergence\n",
    "\\[ V_{k+1}(s) = \\max_a [R(s) + \\gamma × V_k(s')] \\]\n",
    "\n",
    "The algorithm converges when the maximum change in value function falls below a threshold:\n",
    "\\[ \\max_s |V_{k+1}(s) - V_k(s)| < \\theta \\]\n",
    "\n",
    "---\n",
    "Talking Points:\n",
    "\n",
    "I performed two iterations of Value Iteration on a 2×2 gridworld with rewards of 5, 10, 1, and 2 for states s1, s2, s3, and s4 respectively, using a discount factor of 0.9.\n",
    "\n",
    "**Iteration 1** produced values equal to the immediate rewards (5.0, 10.0, 1.0, 2.0) since all successor states started at zero. **Iteration 2** showed significant value propagation: s1 increased to 14.0, s2 to 19.0, s3 to 5.5, and s4 to 11.0.\n",
    "\n",
    "The optimal policy emerged clearly—all states gravitate toward s2, which has the highest immediate reward. State s1's best action is to move right to s2, s3 should move up toward s1 (then to s2), and s4 should move up directly to s2. State s2 itself benefits from staying put.\n",
    "\n",
    "This demonstrates how value iteration uses the Bellman optimality equation to propagate future reward information backward through the state space, allowing the agent to discover that maximizing long-term value means navigating to and remaining in high-reward regions.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
